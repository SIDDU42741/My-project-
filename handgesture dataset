import cv2
import mediapipe as mp
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.7)

# Example dataset (gesture name -> image paths)
gesture_data = {
    "thumbs_up": ["data/thumbs_up1.jpg", "data/thumbs_up2.jpg", "data/thumbs_up3.jpg"],
    "peace": ["data/peace1.jpg", "data/peace2.jpg", "data/peace3.jpg"],
    "ok": ["data/ok1.jpg", "data/ok2.jpg", "data/ok3.jpg"]
}

# Feature extraction function
def extract_landmarks(image_path):
    image = cv2.imread(image_path)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    result = hands.process(image_rgb)
    
    if result.multi_hand_landmarks:
        hand_landmarks = result.multi_hand_landmarks[0]
        landmarks = []
        for lm in hand_landmarks.landmark:
            landmarks.append(lm.x)
            landmarks.append(lm.y)
        return landmarks
    else:
        return None

# Prepare dataset
X = []
y = []

for label, image_paths in gesture_data.items():
    for path in image_paths:
        features = extract_landmarks(path)
        if features:
            X.append(features)
            y.append(label)

X = np.array(X)
y = np.array(y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train SVM classifier
model = SVC(kernel='rbf')
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))

# --- Real-time prediction ---
cap = cv2.VideoCapture(0)
with mp_hands.Hands(min_detection_confidence=0.7, max_num_hands=1) as hands_live:
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = hands_live.process(image_rgb)
        
        if result.multi_hand_landmarks:
            for hand_landmarks in result.multi_hand_landmarks:
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                landmarks = []
                for lm in hand_landmarks.landmark:
                    landmarks.append(lm.x)
                    landmarks.append(lm.y)
                
                prediction = model.predict([landmarks])
                cv2.putText(frame, prediction[0], (50, 50), cv2.FONT_HERSHEY_SIMPLEX,
                            1, (0, 255, 0), 2, cv2.LINE_AA)
        
        cv2.imshow("Hand Gesture Recognition", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

cap.release()
cv2.destroyAllWindows()
